ğŸš€ What is Gradient Descent?
Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the lowest point of a loss function. In ML, it's used to optimize model parameters (like weights in neural networks) to minimize errors.

Imagine you're hiking down a mountain blindfolded, feeling the slope with your feet. You take small steps downhill, ensuring you donâ€™t trip. Thatâ€™s gradient descentâ€”step by step, moving towards the minimum loss.


ğŸ“Œ Most Important Points .
1ï¸âƒ£ Cost Function (Loss Function)
A mathematical function that tells how bad the model is performing.
Example: Mean Squared Error (MSE) in regression, Cross-Entropy Loss in classification.
2ï¸âƒ£ Gradient (Slope)
The partial derivative of the cost function. It tells the direction and magnitude of change.
A high gradient means we are far from the minimum; a low gradient means we are close.
3ï¸âƒ£ Learning Rate (Î±)
Controls the size of each step.
Too high â†’ Overshoots the minimum (never converges).
Too low â†’ Takes forever to converge.
4ï¸âƒ£ Convergence
When the model reaches a point where updates are negligible, meaning it's at or near the minimum loss.

ğŸƒâ€â™‚ï¸ Types of Gradient Descent
1ï¸âƒ£ Batch Gradient Descent (BGD)
Uses the entire dataset to compute gradients.
âœ… Stable convergence, good for convex functions.
âŒ Slow for large datasets.
2ï¸âƒ£ Stochastic Gradient Descent (SGD)
Updates the model using one random sample at a time.
âœ… Faster updates, works well for online learning.
âŒ High variance, can oscillate.
3ï¸âƒ£ Mini-Batch Gradient Descent
Uses a small subset (mini-batch) of data to compute gradients.
âœ… Balances speed and stability.
âŒ Choosing batch size requires tuning.
